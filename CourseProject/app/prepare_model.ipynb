{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, classification_report, precision_recall_curve, confusion_matrix\n",
    "\n",
    "import dill\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ablaze</td>\n",
       "      <td></td>\n",
       "      <td>Communal violence in Bhainsa, Telangana. \"Ston...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ablaze</td>\n",
       "      <td></td>\n",
       "      <td>Telangana: Section 144 has been imposed in Bha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>New York City</td>\n",
       "      <td>Arsonist sets cars ablaze at dealership https:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ablaze</td>\n",
       "      <td>Morgantown, WV</td>\n",
       "      <td>Arsonist sets cars ablaze at dealership https:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ablaze</td>\n",
       "      <td></td>\n",
       "      <td>\"Lord Jesus, your love brings freedom and pard...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11365</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>Blue State in a red sea</td>\n",
       "      <td>Media should have warned us well in advance. T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11366</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>arohaonces</td>\n",
       "      <td>i feel directly attacked ðŸ’€ i consider moonbin ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11367</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>ðŸ‡µðŸ‡­</td>\n",
       "      <td>i feel directly attacked ðŸ’€ i consider moonbin ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11368</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>auroraborealis</td>\n",
       "      <td>ok who remember \"outcast\" nd the \"dora\" au?? T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11369</th>\n",
       "      <td>wrecked</td>\n",
       "      <td></td>\n",
       "      <td>Jake Corway wrecked while running 14th at IRP.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11370 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       keyword                 location  \\\n",
       "0       ablaze                            \n",
       "1       ablaze                            \n",
       "2       ablaze            New York City   \n",
       "3       ablaze           Morgantown, WV   \n",
       "4       ablaze                            \n",
       "...        ...                      ...   \n",
       "11365  wrecked  Blue State in a red sea   \n",
       "11366  wrecked               arohaonces   \n",
       "11367  wrecked                       ðŸ‡µðŸ‡­   \n",
       "11368  wrecked           auroraborealis   \n",
       "11369  wrecked                            \n",
       "\n",
       "                                                    text  target  \n",
       "0      Communal violence in Bhainsa, Telangana. \"Ston...       1  \n",
       "1      Telangana: Section 144 has been imposed in Bha...       1  \n",
       "2      Arsonist sets cars ablaze at dealership https:...       1  \n",
       "3      Arsonist sets cars ablaze at dealership https:...       1  \n",
       "4      \"Lord Jesus, your love brings freedom and pard...       0  \n",
       "...                                                  ...     ...  \n",
       "11365  Media should have warned us well in advance. T...       0  \n",
       "11366  i feel directly attacked ðŸ’€ i consider moonbin ...       0  \n",
       "11367  i feel directly attacked ðŸ’€ i consider moonbin ...       0  \n",
       "11368  ok who remember \"outcast\" nd the \"dora\" au?? T...       0  \n",
       "11369     Jake Corway wrecked while running 14th at IRP.       1  \n",
       "\n",
       "[11370 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'data'\n",
    "model_path = 'models'\n",
    "\n",
    "df = pd.read_csv(data_path + '/tweets.csv').drop('id', 1).fillna('')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                    3418\n",
       "United States         96\n",
       "Australia             83\n",
       "London, England       81\n",
       "UK                    77\n",
       "                    ... \n",
       "D(M)V                  1\n",
       "in mista's pants       1\n",
       "your stomach           1\n",
       "burritoblanket         1\n",
       "Webster, MA            1\n",
       "Name: location, Length: 4505, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "thunderstorm     93\n",
       "flattened        88\n",
       "mass%20murder    86\n",
       "stretcher        86\n",
       "drown            83\n",
       "                 ..\n",
       "electrocuted     16\n",
       "rainstorm        11\n",
       "deluged          10\n",
       "siren            10\n",
       "tsunami           6\n",
       "Name: keyword, Length: 219, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['keyword'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('target', axis=1)\n",
    "y=df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=0)\n",
    "\n",
    "X_test.to_csv(data_path + \"/X_test.csv\", index=None)\n",
    "y_test.to_csv(data_path + \"/y_test.csv\", index=None)\n",
    "X_train.to_csv(data_path + \"/X_train.csv\", index=None)\n",
    "y_train.to_csv(data_path + \"/y_train.csv\", index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.column]\n",
    "\n",
    "\n",
    "class OHEEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        self.columns = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.columns = [col for col in pd.get_dummies(X, prefix=self.key).columns]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.get_dummies(X, prefix=self.key)\n",
    "        x_columns = [col for col in X.columns]\n",
    "        for col_ in self.columns:\n",
    "            if col_ not in x_columns:\n",
    "                 X[col_] = 0\n",
    "        return X[self.columns]\n",
    "\n",
    "    \n",
    "    \n",
    "def run(name, pipeline, **fit_params):\n",
    "    print(f'{name}: Cross-validation ...')\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=3, scoring='roc_auc')\n",
    "    cv_score = np.mean(cv_scores)\n",
    "\n",
    "    print(f'{name}: Training ...')\n",
    "    pipeline.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "    dill_path = model_path + '/' + name + '.dill'\n",
    "\n",
    "    print(f'{name}: Storing to {dill_path} ...')\n",
    "    with open(dill_path, 'wb') as out_strm:\n",
    "        dill.dump(pipeline, out_strm)\n",
    "\n",
    "    print(f'{name}: Loading from {dill_path} ...')\n",
    "    with open(dill_path, 'rb') as in_strm:\n",
    "        pipeline = dill.load(in_strm)    \n",
    "    \n",
    "    print(f'{name}: Testing loaded pipeline ...')\n",
    "    preds = pipeline.predict_proba(X_test)[:, 1]\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
    "\n",
    "    fscore = (2 * precision * recall) / (precision + recall)\n",
    "    ix = np.argmax(fscore)\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_test, preds>thresholds[ix])\n",
    "    \n",
    "    TN = cnf_matrix[0][0]\n",
    "    FN = cnf_matrix[1][0]\n",
    "    TP = cnf_matrix[1][1]\n",
    "    FP = cnf_matrix[0][1]\n",
    "    \n",
    "    metrics = (thresholds[ix], fscore[ix], precision[ix], recall[ix], cv_score, cnf_matrix)\n",
    "    metrics_string = 'Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f, CV score=%.3f' % metrics[0:5]\n",
    "    results[name] = (metrics_string,) + metrics\n",
    "    return pipeline\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=None,\n",
       "             transformer_list=[('text',\n",
       "                                Pipeline(memory=None,\n",
       "                                         steps=[('text_selector',\n",
       "                                                 FeatureSelector(column='text')),\n",
       "                                                ('text_tfidf',\n",
       "                                                 TfidfVectorizer(analyzer='word',\n",
       "                                                                 binary=False,\n",
       "                                                                 decode_error='strict',\n",
       "                                                                 dtype=<class 'numpy.float64'>,\n",
       "                                                                 encoding='utf-8',\n",
       "                                                                 input='content',\n",
       "                                                                 lowercase=True,\n",
       "                                                                 max_df=1.0,\n",
       "                                                                 max_features=100,\n",
       "                                                                 min_df=1,\n",
       "                                                                 ngram_range=(1,\n",
       "                                                                              1),\n",
       "                                                                 norm='l2'...\n",
       "                                                                 dtype=<class 'numpy.float64'>,\n",
       "                                                                 encoding='utf-8',\n",
       "                                                                 input='content',\n",
       "                                                                 lowercase=True,\n",
       "                                                                 max_df=1.0,\n",
       "                                                                 max_features=5,\n",
       "                                                                 min_df=1,\n",
       "                                                                 ngram_range=(1,\n",
       "                                                                              1),\n",
       "                                                                 norm='l2',\n",
       "                                                                 preprocessor=None,\n",
       "                                                                 smooth_idf=True,\n",
       "                                                                 stop_words='english',\n",
       "                                                                 strip_accents='unicode',\n",
       "                                                                 sublinear_tf=True,\n",
       "                                                                 token_pattern='\\\\w{1,}',\n",
       "                                                                 tokenizer=None,\n",
       "                                                                 use_idf=True,\n",
       "                                                                 vocabulary=None))],\n",
       "                                         verbose=False))],\n",
       "             transformer_weights=None, verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = FeatureUnion([\n",
    "                           ('text', Pipeline([('text_selector', FeatureSelector(column='text')), \n",
    "                                             ('text_tfidf', TfidfVectorizer(sublinear_tf=True,\n",
    "                                                            strip_accents='unicode',\n",
    "                                                            analyzer='word',\n",
    "                                                            token_pattern=r'\\w{1,}',\n",
    "                                                            stop_words='english',\n",
    "                                                            ngram_range=(1, 1),\n",
    "                                                            max_features=100)), \n",
    "                                             ])),\n",
    "                           ('keyword', Pipeline([('keyword_selector', FeatureSelector(column='keyword')), \n",
    "                                                 ('ohe', OHEEncoder(key='keyword'))\n",
    "                                             ])),\n",
    "                           ('location', Pipeline([('location_selector', FeatureSelector(column='location')), \n",
    "                                             ('text_tfidf', TfidfVectorizer(sublinear_tf=True,\n",
    "                                                            strip_accents='unicode',\n",
    "                                                            analyzer='word',\n",
    "                                                            token_pattern=r'\\w{1,}',\n",
    "                                                            stop_words='english',\n",
    "                                                            ngram_range=(1, 1),\n",
    "                                                            max_features=5)), \n",
    "                                             ])),\n",
    "                        ])\n",
    "features.fit_transform(X_train)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression: Cross-validation ...\n",
      "LogisticRegression: Training ...\n",
      "LogisticRegression: Storing to models/LogisticRegression.dill ...\n",
      "LogisticRegression: Loading from models/LogisticRegression.dill ...\n",
      "LogisticRegression: Testing loaded pipeline ...\n",
      "RandomForestClassifier: Cross-validation ...\n",
      "RandomForestClassifier: Training ...\n",
      "RandomForestClassifier: Storing to models/RandomForestClassifier.dill ...\n",
      "RandomForestClassifier: Loading from models/RandomForestClassifier.dill ...\n",
      "RandomForestClassifier: Testing loaded pipeline ...\n",
      "CatBoostClassifier: Cross-validation ...\n",
      "CatBoostClassifier: Training ...\n",
      "CatBoostClassifier: Storing to models/CatBoostClassifier.dill ...\n",
      "CatBoostClassifier: Loading from models/CatBoostClassifier.dill ...\n",
      "CatBoostClassifier: Testing loaded pipeline ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('features',\n",
       "                 FeatureUnion(n_jobs=None,\n",
       "                              transformer_list=[('text',\n",
       "                                                 Pipeline(memory=None,\n",
       "                                                          steps=[('text_selector',\n",
       "                                                                  FeatureSelector(column='text')),\n",
       "                                                                 ('text_tfidf',\n",
       "                                                                  TfidfVectorizer(analyzer='word',\n",
       "                                                                                  binary=False,\n",
       "                                                                                  decode_error='strict',\n",
       "                                                                                  dtype=<class 'numpy.float64'>,\n",
       "                                                                                  encoding='utf-8',\n",
       "                                                                                  input='content',\n",
       "                                                                                  lowercase=True,\n",
       "                                                                                  max_df=1.0,\n",
       "                                                                                  max_features=...\n",
       "                                                                                  min_df=1,\n",
       "                                                                                  ngram_range=(1,\n",
       "                                                                                               1),\n",
       "                                                                                  norm='l2',\n",
       "                                                                                  preprocessor=None,\n",
       "                                                                                  smooth_idf=True,\n",
       "                                                                                  stop_words='english',\n",
       "                                                                                  strip_accents='unicode',\n",
       "                                                                                  sublinear_tf=True,\n",
       "                                                                                  token_pattern='\\\\w{1,}',\n",
       "                                                                                  tokenizer=None,\n",
       "                                                                                  use_idf=True,\n",
       "                                                                                  vocabulary=None))],\n",
       "                                                          verbose=False))],\n",
       "                              transformer_weights=None, verbose=False)),\n",
       "                ('classifier',\n",
       "                 <catboost.core.CatBoostClassifier object at 0x000001F4E1033388>)],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import catboost as catb\n",
    "\n",
    "def classifier(clf):\n",
    "    return Pipeline([ ('features', features), ('classifier', clf) ])\n",
    "\n",
    "results={}\n",
    "\n",
    "run(\"LogisticRegression\", classifier(LogisticRegression(C=0.1, solver='sag', random_state=42)))\n",
    "run(\"RandomForestClassifier\", classifier(RandomForestClassifier(random_state=42)))\n",
    "run(\"CatBoostClassifier\", classifier(catb.CatBoostClassifier(silent=True, random_state=42)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LogisticRegression': ('Best Threshold=0.228861, F-Score=0.523, Precision=0.441, Recall=0.644, CV score=0.804',\n",
       "  0.22886101526908317,\n",
       "  0.5233644859813084,\n",
       "  0.4409448818897638,\n",
       "  0.6436781609195402,\n",
       "  0.8040616954279547,\n",
       "  array([[411,  71],\n",
       "         [ 32,  55]], dtype=int64)),\n",
       " 'RandomForestClassifier': ('Best Threshold=0.422310, F-Score=0.553, Precision=0.646, Recall=0.483, CV score=0.788',\n",
       "  0.4223095238095238,\n",
       "  0.5526315789473685,\n",
       "  0.6461538461538462,\n",
       "  0.4827586206896552,\n",
       "  0.7883604805988752,\n",
       "  array([[459,  23],\n",
       "         [ 46,  41]], dtype=int64)),\n",
       " 'CatBoostClassifier': ('Best Threshold=0.363309, F-Score=0.517, Precision=0.633, Recall=0.437, CV score=0.802',\n",
       "  0.3633087446456145,\n",
       "  0.5170068027210883,\n",
       "  0.6333333333333333,\n",
       "  0.4367816091954023,\n",
       "  0.801917711652648,\n",
       "  array([[460,  22],\n",
       "         [ 50,  37]], dtype=int64))}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
